
@inproceedings{adebayoSanityChecksSaliency2018,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html},
  urldate = {2022-08-18},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/W2UHPF67/Adebayo et al. - 2018 - Sanity Checks for Saliency Maps.pdf}
}

@inproceedings{anconaBetterUnderstandingGradientbased2018,
  title = {Towards Better Understanding of Gradient-Based Attribution Methods for {{Deep Neural Networks}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2018, {{Vancouver}}, {{BC}}, {{Canada}}, {{April}} 30 - {{May}} 3, 2018, {{Conference Track Proceedings}}},
  author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
  date = {2018},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=Sy21R9JAW},
  urldate = {2022-08-18}
}

@online{CIFAR10CIFAR100Datasets,
  title = {{{CIFAR-10}} and {{CIFAR-100}} Datasets},
  url = {https://www.cs.toronto.edu/~kriz/cifar.html},
  urldate = {2022-09-02},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/DJS7IAAD/cifar.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/BNJT275L/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/Z84NSPSX/7780459.html}
}

@inproceedings{hookerBenchmarkInterpretabilityMethods2019,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html},
  urldate = {2022-08-18},
  abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/8FPGGC2Q/Hooker et al. - 2019 - A Benchmark for Interpretability Methods in Deep N.pdf}
}

@online{ImageNet,
  title = {{{ImageNet}}},
  url = {https://www.image-net.org/},
  urldate = {2022-08-22},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/LE2EUFM7/www.image-net.org.html}
}

@software{Imagenette2022,
  title = {Imagenette},
  date = {2022-09-01T15:54:55Z},
  origdate = {2019-03-06T01:58:45Z},
  url = {https://github.com/fastai/imagenette},
  urldate = {2022-09-02},
  abstract = {A smaller subset of 10 easily classified classes from Imagenet, and a little more French},
  organization = {{fast.ai}}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2022-08-22},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/T7FCIMVM/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{lecunConvolutionalNetworksImages,
  title = {Convolutional {{Networks}} for {{Images}}, {{Speech}}, and {{Time-Series}}},
  author = {LeCun, Yann and Bengio, Yoshua and Laboratories, T Bell},
  pages = {14},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/AYXU5SUC/LeCun et al. - Convolutional Networks for Images, Speech, and Tim.pdf}
}

@online{MNISTHandwrittenDigit,
  title = {{{MNIST}} Handwritten Digit Database, {{Yann LeCun}}, {{Corinna Cortes}} and {{Chris Burges}}},
  url = {http://yann.lecun.com/exdb/mnist/},
  urldate = {2022-08-23},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/WYM2DS42/mnist.html}
}

@article{petsiukRISERandomizedInput,
  title = {{{RISE}}: {{Randomized Input Sampling}} for {{Explanation}} of {{Black-box Models}}},
  author = {Petsiuk, Vitali},
  pages = {13},
  abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model’s prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on blackbox models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/XDI39SEY/Petsiuk - RISE Randomized Input Sampling for Explanation of.pdf}
}

@misc{shrikumarNotJustBlack2017,
  title = {Not {{Just}} a {{Black Box}}: {{Learning Important Features Through Propagating Activation Differences}}},
  shorttitle = {Not {{Just}} a {{Black Box}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
  date = {2017-04-11},
  number = {arXiv:1605.01713},
  eprint = {1605.01713},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.01713},
  url = {http://arxiv.org/abs/1605.01713},
  urldate = {2022-08-23},
  abstract = {Note: This paper describes an older version of DeepLIFT. See https://arxiv.org/abs/1704.02685 for the newer version. Original abstract follows: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/G95V459D/Shrikumar et al. - 2017 - Not Just a Black Box Learning Important Features .pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/IDVY7IET/1605.html}
}

@misc{simonyanDeepConvolutionalNetworks2014a,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  number = {arXiv:1312.6034},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-08-22},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/E3DSJCPC/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/SQA9GEEP/1312.html}
}

@article{singhThinkPositiveInterpretable2022,
  title = {Think Positive: {{An}} Interpretable Neural Network for Image Recognition},
  shorttitle = {Think Positive},
  author = {Singh, Gurmail},
  date = {2022-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {151},
  pages = {178--189},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2022.03.034},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608022001125},
  urldate = {2022-08-22},
  abstract = {The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X-rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48\%, 0.99, 0.99 and 0.99, respectively.},
  langid = {english},
  keywords = {COVID-19,CT-scan,Interpretable,Pneumonia,Prototypes},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/6AYGBYPE/Singh - 2022 - Think positive An interpretable neural network fo.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/GBDNMV9S/S0893608022001125.html}
}

@article{smilkovSmoothGradRemovingNoise,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  pages = {10},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/PN4IYK9X/Smilkov et al. - SmoothGrad removing noise by adding noise.pdf}
}

@misc{springenbergStrivingSimplicityAll2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  number = {arXiv:1412.6806},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6806},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2022-08-18},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/KI9GBIHN/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/R4TMJUUM/1412.html}
}

@misc{springenbergStrivingSimplicityAll2015a,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  number = {arXiv:1412.6806},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6806},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2022-08-23},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/PTQ5MMT9/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/UJKBJYTU/1412.html}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  date = {2017-07-17},
  pages = {3319--3328},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/sundararajan17a.html},
  urldate = {2022-08-23},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/J9CJHJNA/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/Q6IBMRLG/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}


