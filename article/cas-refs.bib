@InProceedings{Adebayo2018,
  author    = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS)},
  title     = {Sanity checks for saliency maps},
  year      = {2018},
  address   = {Montr\'{e}al, Canada},
  month     = dec,
  pages     = {9525--9536},
  publisher = {Curran Associates, Inc.},
  abstract  = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
}

@InProceedings{Ancona2018,
  author    = {Ancona, Marco and Ceolini, Enea and {\"O}ztireli, Cengiz and Gross, Markus},
  booktitle = {6th Int. Conf. Learning Representations, ({ICLR})},
  title     = {Towards better understanding of gradient-based attribution methods for deep neural networks},
  year      = {2018},
  address   = {Vancouver, BC, Canada},
  month     = {{30}} # apr # {{--3}#may},
}

@Misc{CIFAR10CIFAR100Datasets,
  author = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  title  = {{{CIFAR-10}} and {{CIFAR-100}} Datasets},
  file   = {C\:\\Users\\osllo\\Zotero\\storage\\DJS7IAAD\\cifar.html},
  url    = {https://www.cs.toronto.edu/\textasciitilde kriz/cifar.html},
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C\:\\Users\\osllo\\Zotero\\storage\\BNJT275L\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\osllo\\Zotero\\storage\\Z84NSPSX\\7780459.html}
}

@inproceedings{hookerBenchmarkInterpretabilityMethods2019,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  file = {C\:\\Users\\osllo\\Zotero\\storage\\8FPGGC2Q\\Hooker et al. - 2019 - A Benchmark for Interpretability Methods in Deep N.pdf}
}

@InProceedings{ImageNet,
  author    = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  booktitle = {IEEE Conf. Computer Vision and Pattern Recognition},
  title     = {Image{N}et: {A} large-scale hierarchical image database},
  year      = {2009},
  pages     = {248--255},
  file      = {C\:\\Users\\osllo\\Zotero\\storage\\LE2EUFM7\\www.image-net.org.html},
  url       = {https://www.image-net.org/},
}

@Dataset{Imagenette2022,
  abstract     = {A smaller subset of 10 easily classified classes from Imagenet, and a little more French},
  author       = {Howard, Jeremy},
  organization = {{fast.ai}},
  title        = {Imagenette},
  url          = {https://github.com/fastai/imagenette},
  urldate      = {2022-05-24},
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C\:\\Users\\osllo\\Zotero\\storage\\T7FCIMVM\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@InBook{LeCun1998,
  author    = {LeCun, Yann and Bengio, Yoshua},
  pages     = {255--258},
  publisher = {MIT Press},
  title     = {Convolutional networks for images, speech, and time series},
  year      = {1998},
  address   = {Cambridge, MA, USA},
  month     = oct,
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  date      = {1998},
  doi       = {10.5555/303568.303704},
}

@misc{MNISTHandwrittenDigit,
  title = {{{MNIST}} Handwritten Digit Database, {{Yann LeCun}}, {{Corinna Cortes}} and {{Chris Burges}}},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  file = {C\:\\Users\\osllo\\Zotero\\storage\\WYM2DS42\\mnist.html}
}

@InProceedings{Petsiuk2018,
  author    = {Vitali Petsiuk and Abir Das and Kate Saenko},
  booktitle = {British Machine Vision Conference (BMVC)},
  title     = {{RISE}: Randomized input sampling for explanation of black-box models},
  year      = {2018},
  abstract  = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on blackbox models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.},
  file      = {C\:\\Users\\osllo\\Zotero\\storage\\XDI39SEY\\Petsiuk - RISE Randomized Input Sampling for Explanation of.pdf},
}

@InProceedings{Shrikumar2017,
  author    = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle = {Proc. of the 34th International Conference on Machine Learning (PMLR)},
  title     = {Learning important features through propagating activation differences},
  year      = {2017},
  month     = {06--11 Aug},
  pages     = {3145--3153},
  volume    = {70},
}

@InProceedings{Simonyan2014,
  author    = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  booktitle = {2nd International Conference on Learning Representations (ICLR), April 14--16, Workshop Track Proceedings},
  title     = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
  year      = {2014},
  abstract  = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  location  = {Banff, AB, Canada},
}

@Article{Singh2022,
  author   = {Singh, Gurmail},
  journal  = {Neural Networks},
  title    = {Think positive: {A}n interpretable neural network for image recognition},
  year     = {2022},
  month    = jul,
  pages    = {178--189},
  volume   = {151},
  abstract = {The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X-rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48\%, 0.99, 0.99 and 0.99, respectively.},
  doi      = {10.1016/j.neunet.2022.03.034},
  file     = {C\:\\Users\\osllo\\Zotero\\storage\\6AYGBYPE\\Singh - 2022 - Think positive An interpretable neural network fo.pdf;C\:\\Users\\osllo\\Zotero\\storage\\GBDNMV9S\\S0893608022001125.html},
}

@Article{Smilkov2017,
  author   = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal  = {Computing Research Repository (CoRR)},
  title    = {{{SmoothGrad}}: Removing noise by adding noise},
  year     = {2017},
  pages    = {10},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  file     = {C\:\\Users\\osllo\\Zotero\\storage\\PN4IYK9X\\Smilkov et al. - SmoothGrad removing noise by adding noise.pdf},
  langid   = {english},
}

@InProceedings{Sundarajan2017,
  author    = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  title     = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  year      = {2017},
  month     = jul,
  pages     = {3319--3328},
  publisher = {{PMLR}},
  abstract  = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  file      = {C\:\\Users\\osllo\\Zotero\\storage\\J9CJHJNA\\Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf},
  issn      = {2640-3498},
  langid    = {english},
}

@InProceedings{Zeiler2014,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = {European Conference on Computer Vision},
  title     = {Visualizing and understanding convolutional networks},
  year      = {2014},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {818--833},
  abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  doi       = {10.1007/978-3-319-10590-1\_53},
}

@Article{Lowe2004,
  author  = {David G. Lowe},
  journal = {International Journal of Computer Vision},
  title   = {Distinctive image features from scale-invariant keypoints},
  year    = {2004},
  month   = nov,
  pages   = {91--110},
  volume  = {60},
  doi     = {10.1023/B:VISI.0000029664.99615.94},
}

@Article{Zhang2021,
  author    = {Q. Zhang and X. Wang and Y. Wu and H. Zhou and S. Zhu},
  journal   = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  title     = {Interpretable {CNN}s for object classification},
  year      = {2021},
  month     = oct,
  number    = {10},
  pages     = {3416--3431},
  volume    = {43},
  abstract  = {This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of the CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various architectures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/TPAMI.2020.2982882},
  keywords  = {visualization;semantics;neural networks;task analysis;feature extraction;annotations;benchmark testing},
  publisher = {IEEE Computer Society},
}

@InProceedings{Zhou2015,
  author    = {Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
  booktitle = {3rd International Conference on Learning Representations (ICLR) , May 7--9, Conference Track Proceedings},
  title     = {Object detectors emerge in deep scene {CNN}s},
  year      = {2015},
  address   = {San Diego, CA, USA},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/ZhouKLOT14.bib},
}

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
  title     = {``{W}hy should {I} trust you?'' {E}xplaining the predictions of any classifier},
  year      = {2016},
  address   = {San Diego, CA, USA},
  month     = jun,
  pages     = {97--101},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N16-3020},
}

@Article{Frosst2017,
  author        = {Nicholas Frosst and Geoffrey Hinton},
  journal       = {Computing Research Repository (CoRR)},
  title         = {Distilling a neural network into a soft decision tree},
  year          = {2017},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
}

@InProceedings{Zhou2016,
  author    = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning deep features for discriminative localization},
  year      = {2016},
  address   = {Las Vegas, NV, USA},
  pages     = {2921--2929},
  doi       = {10.1109/CVPR.2016.319},
  eventdate = {2016-06-27/2016-06-30},
}

@Article{Selvaraju2020,
  author  = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  journal = {International Journal of Computer Vision},
  title   = {Grad-{CAM}: {V}isual explanations from deep networks via gradient-based localization},
  year    = {2020},
  month   = feb,
  pages   = {336--359},
  volume  = {128},
  doi     = {10.1007/s11263-019-01228-7},
}

@InProceedings{Springenberg2015,
  author    = {Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin A. Riedmiller},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  title     = {Striving for simplicity: The all convolutional net},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/SpringenbergDBR14.bib},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  url       = {http://arxiv.org/abs/1412.6806},
}

@InProceedings{Chattopadhay2018,
  author    = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  booktitle = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Grad-{CAM}++: Generalized gradient-based visual explanations for deep convolutional networks},
  year      = {2018},
  address   = {Lake Tahoe, NV, USA},
  pages     = {839--847},
  doi       = {10.1109/WACV.2018.00097},
  eventdate = {2018-03-12/2018-03-15},
}

@InProceedings{Li2018,
  author    = {Li, Kunpeng and Wu, Ziyan and Peng, Kuan-Chuan and Ernst, Jan and Fu, Yun},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Tell me where to look: Guided attention inference network},
  year      = {2018},
  pages     = {11966--11976},
  doi       = {10.1109/CVPR.2018.00960},
}

@Article{An2022,
  author         = {An, Junkang and Joe, Inwhee},
  journal        = {Applied Sciences},
  title          = {Attention map-guided visual explanations for deep neural networks},
  year           = {2022},
  issn           = {2076-3417},
  number         = {8},
  volume         = {12},
  abstract       = {Deep neural network models perform well in a variety of domains, such as computer vision, recommender systems, natural language processing, and defect detection. In contrast, in areas such as healthcare, finance, and defense, deep neural network models, due to their lack of explainability, are not trusted by users. In this paper, we focus on attention-map-guided visual explanations for deep neural networks. We employ an attention mechanism to find the most important region of an input image. The Grad-CAM method is used to extract the feature map for deep neural networks, and then the attention mechanism is used to extract the high-level attention maps. The attention map, which highlights the important region in the image for the target class, can be seen as a visual explanation of a deep neural network. We evaluate our method using two common metrics: average drop and percentage increase. For a more effective experiment, we also propose a new metric to evaluate our method. The experiments were carried out to show that the proposed method works better than the state-of-the-art explainable artificial intelligence method. Our approach can provide a lower average drop and higher percent increase when compared to other methods and find a more explanatory region, especially in the first twenty percent region of the input image.},
  article-number = {3846},
  doi            = {10.3390/app12083846},
}

@InProceedings{Liu2021,
  author    = {Zixuan Liu and Ehsan Adeli and Kilian M. Pohl and Qingyu Zhao},
  booktitle = {Information Processing in Medical Imaging},
  title     = {Going beyond saliency maps: Training deep models to interpret deep models},
  year      = {2021},
  editor    = {Feragen, Aasa and Sommer, Stefan and Schnabel, Julia and Nielsen, Mads},
  month     = jun,
  pages     = {71--82},
  volume    = {12729},
  doi       = {10.1007/978-3-030-78191-0_6},
}

@InProceedings{Kim2019,
  author    = {Kim, Beomsu and Seo, Junghoon and Jeon, Seunghyeon and Koo, Jamyoung and Choe, Jeongyeol and Jeon, Taegyun},
  booktitle = {IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  title     = {Why are Saliency Maps Noisy? Cause of and Solution to Noisy Saliency Maps},
  year      = {2019},
  pages     = {4149--4157},
  doi       = {10.1109/ICCVW.2019.00510},
}

@Article{Liu2022,
  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christopher Feichtenhofer and Trevor Darrell and Saining Xie},
  journal = {IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)},
  title   = {A {ConvNet} for the 2020s},
  year    = {2022},
  pages   = {11966--11976},
}

@Comment{jabref-meta: databaseType:bibtex;}
